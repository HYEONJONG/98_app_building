{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM75zo4bWN0bNG4NgxQO28c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HYEONJONG/98_app_building/blob/master/chatper2_DQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chapter 2\n",
        "import gymnasium as gym\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "env.action_space\n",
        "env.action_space.n\n",
        "[env.action_space.sample() for _ in range(10)]\n",
        "env.observation_space   # cart position, cart velocity, pole angle, pole angular velocity\n",
        "env.observation_space.shape\n",
        "\n",
        "env.reset(seed = 100)\n",
        "env.step(0)\n",
        "env.step(1)             # new state, reward, terminated, truncated, additional data\n",
        "\n",
        "# A randeom agent\n",
        "class RandomAgent:\n",
        "  def __init__(self):\n",
        "    self.env = gym.make('CartPole-v1')\n",
        "  def play(self, episode = 1):\n",
        "    self.trewards=list()\n",
        "    for e in range(episode):\n",
        "      self.env.reset()\n",
        "      for step in range(1,100):\n",
        "        a = self.env.action_space.sample()\n",
        "        state, reward, done, truc, info=self.env.step(a)\n",
        "        if done:\n",
        "          self.trewards.append(step)\n",
        "          break\n",
        "\n",
        "ra = RandomAgent()\n",
        "ra.play(15)\n",
        "ra.trewards\n",
        "round(sum(ra.trewards)/len(ra.trewards),2)"
      ],
      "metadata": {
        "id": "oNApIJQNL5AY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8920cd90-0b98-46a7-a77c-5801fb0f85eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.07"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DQL Agent\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "BcaByaiaUqGI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
        "os.environ['PYTHONHASHSEED']='0'\n",
        "\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()                # spped up the training of the neural network\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "randon.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "class DQLAgent:\n",
        "  def __init__(self):\n",
        "    self.epsilon=1.0\n",
        "    self.epsilon_decay=0.9975\n",
        "    self.epsilon_min=0.1\n",
        "    self.memory=deque(maxlen=2000)\n",
        "    self.batch_size=32\n",
        "    self.gamma=0.0\n",
        "    self.trewards=0\n",
        "    self.max_trewards=0\n",
        "    self._create_model()\n",
        "    self.env=gym.make('CartPole-v1')\n",
        "  def _create_model(self):\n",
        "    self.model=Sequential()\n",
        "    self.model.add(Dense(24, activation='relu',input_dim=4))\n",
        "    self.model.add(Dense(24, activation='relu'))\n",
        "    self.model.add(Dense(2, activation = 'linear'))\n",
        "    self.model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "class DQLAgent(DQLAgent):\n",
        "  def act(self, state):\n",
        "    if random.random() < self.epsilon:\n",
        "      return self.env.action_space.sample()          # random action\n",
        "    return np.argmax(self.model.predict(state)[0])   # choose optimal policy\n",
        "  def replay(self):\n",
        "    batch=random.sample(self.memory, self.batch_size)\n",
        "    for state, action, reward, done in batch:\n",
        "      if not done:\n",
        "        reward += self.gamma*np.amax(self.model.predict(next_state)[0])  # ombine immediate and discounted future reward\n",
        "      target=self.model.predict(state)\n",
        "      target[0,action]=reward\n",
        "      self.model.fit(state, target, epochs=2, verbose=False)             # train/updates the DNN to account for the updated value\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *=self.epsilon_decay\n",
        "\n",
        "class DQLAgent(DQLAgent):\n",
        "  def learn(self, episodes):\n",
        "    for e in range(1, episodes + 1):\n",
        "      state, _ = self.env.reset()\n",
        "      state = np.reshape(state, [1,4])\n",
        "      for f in range(1,50000):\n",
        "        action = self.act(state)         # cation is chose given the current state\n",
        "        next_state, reward, done, trunc, _ = self.env.step(action)\n",
        "        next_state = np.reshape(next_state, [1,4])\n",
        "        self.memory.append([state, action, next_state, reward, done])\n",
        "        state = next_state\n",
        "        if done or trunc:\n",
        "          self.trewards=max(self.max_treward, f)\n",
        "          templ = f'episode={e:4d} |treward={f:4d}'\n",
        "          templ += f'|max={self.max_treward:4d}'\n",
        "          print(templ, end='\\r')\n",
        "          break\n",
        "      if len(self.memory)> self.batch_size:\n",
        "                self.replay()\n",
        "  print()\n",
        "\n",
        "class DQLAgent(DQLAgent):\n",
        "  def test(self, episodes):\n",
        "    for e in range(1, episodes +1):\n",
        "      state, _ = self.env.reset()\n",
        "\n",
        "      state = np.reshape(state, [1,4])\n",
        "      for f in range(1,5001):\n",
        "        action = np.argmax(self.model.predict(state)[0])\n",
        "        state, reward, done, trun, _= self.env.step(action)\n",
        "        state = np.reshape(state[1,4])\n",
        "        if done or trunc:\n",
        "          print(f, end=' ')\n",
        "          break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeTUqDWzamdJ",
        "outputId": "7d8354c2-d1c4-4940-e518-c2b523af7c9f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQLAgent()\n",
        "# %time agent.learn(1500)\n",
        "# episode=1500 | treward = 224 | max = 500\n",
        "# CPU times: user 1min 52s, sys:21.7 s, total: 2min 14s\n",
        "# Wall time: 1min 46s\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "CUkHzce-aqDq",
        "outputId": "d57aaac4-b149-43c0-b4f9-5bf2a225a65b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DQLAgent._create_model() takes 0 positional arguments but 1 was given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2828592054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQLAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# %time agent.learn(1500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# episode=1500 | treward = 224 | max = 500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# CPU times: user 1min 52s, sys:21.7 s, total: 2min 14s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Wall time: 1min 46s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4015371609.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_trewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DQLAgent._create_model() takes 0 positional arguments but 1 was given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.epsilon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "bbOhPw0nfqUV",
        "outputId": "dca6da48-edc2-41cd-ea2f-537a7f901411"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-689352877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15r_hzmSmPhS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}