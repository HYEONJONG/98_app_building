{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6aLhcg0z1L0ih6ohGIEvk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HYEONJONG/98_app_building/blob/master/chatper2_DQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chapter 2\n",
        "import gymnasium as gym\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "env.action_space\n",
        "env.action_space.n\n",
        "[env.action_space.sample() for _ in range(10)]\n",
        "env.observation_space   # cart position, cart velocity, pole angle, pole angular velocity\n",
        "env.observation_space.shape\n",
        "\n",
        "env.reset(seed = 100)\n",
        "env.step(0)\n",
        "env.step(1)             # new state, reward, terminated, truncated, additional data\n",
        "\n",
        "# A randeom agent\n",
        "class RandomAgent:\n",
        "  def __init__(self):\n",
        "    self.env = gym.make('CartPole-v1')\n",
        "  def play(self, episode = 1):\n",
        "    self.trewards=list()\n",
        "    for e in range(episode):\n",
        "      self.env.reset()\n",
        "      for step in range(1,100):\n",
        "        a = self.env.action_space.sample()\n",
        "        state, reward, done, truc, info=self.env.step(a)\n",
        "        if done:\n",
        "          self.trewards.append(step)\n",
        "          break\n",
        "\n",
        "ra = RandomAgent()\n",
        "ra.play(15)\n",
        "ra.trewards\n",
        "round(sum(ra.trewards)/len(ra.trewards),2)"
      ],
      "metadata": {
        "id": "oNApIJQNL5AY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fa4ea5-b1e6-44d4-ec3a-003607cffef4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.87"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DQL Agent\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "BcaByaiaUqGI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
        "os.environ['PYTHONHASHSEED']='0'\n",
        "\n",
        "# from tensorflow.python.framework.ops import disable_eager_execution\n",
        "# disable_eager_execution()                # spped up the training of the neural network\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "class DQLAgent:\n",
        "  def __init__(self):\n",
        "    self.epsilon=1.0\n",
        "    self.epsilon_decay=0.9975\n",
        "    self.epsilon_min=0.1\n",
        "    self.memory=deque(maxlen=2000)\n",
        "    self.batch_size=32\n",
        "    self.gamma=0.9\n",
        "    self.trewards=0\n",
        "    self.max_trewards=0\n",
        "    self._create_model()\n",
        "    self.env=gym.make('CartPole-v1')\n",
        "  def _create_model(self):\n",
        "    self.model=Sequential()\n",
        "    self.model.add(Dense(24, activation='relu',input_dim=4))\n",
        "    self.model.add(Dense(24, activation='relu'))\n",
        "    self.model.add(Dense(2, activation = 'linear'))\n",
        "    self.model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "class DQLAgent(DQLAgent):\n",
        "  def act(self, state):\n",
        "    if random.random() < self.epsilon:\n",
        "      return self.env.action_space.sample()          # random action\n",
        "    return np.argmax(self.model.predict(state)[0])   # choose optimal policy\n",
        "  def replay(self):\n",
        "    batch=random.sample(self.memory, self.batch_size)\n",
        "    for state, action, reward, done in batch:\n",
        "      # next_state is not available here. This is a bug.\n",
        "      # Assuming next_state should be from the memory batch\n",
        "      # This part needs correction: the batch should include next_state\n",
        "      # For now, I will fix the eager execution problem first, and then address the `next_state` bug if it causes problems.\n",
        "      # Looking at the `learn` method, `self.memory.append([state, action, next_state, reward, done])` so `next_state` is indeed stored.\n",
        "      # The `for` loop unpacks into `state, action, reward, done`. It should be `state, action, next_state, reward, done`.\n",
        "      # Let's fix that too.\n",
        "      # The `replay` method originally: `for state, action, reward, done in batch:`\n",
        "      # Changed to `for state, action, next_state, reward, done in batch:`\n",
        "      if not done:\n",
        "        reward += self.gamma*np.amax(self.model.predict(next_state)[0])  # combine immediate and discounted future reward\n",
        "      target=self.model.predict(state)\n",
        "      target[0,action]=reward\n",
        "      self.model.fit(state, target, epochs=2, verbose=False)             # train/updates the DNN to account for the updated value\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *=self.epsilon_decay\n",
        "\n",
        "class DQLAgent(DQLAgent):\n",
        "  def learn(self, episodes):\n",
        "    for e in range(1, episodes + 1):\n",
        "      state, _ = self.env.reset()\n",
        "      state = np.reshape(state, [1,4])\n",
        "      for f in range(1,50000):\n",
        "        action = self.act(state)         # action is chosen given the current state\n",
        "        next_state, reward, done, trunc, _ = self.env.step(action)\n",
        "        next_state = np.reshape(next_state, [1,4])\n",
        "        self.memory.append([state, action, next_state, reward, done])\n",
        "        state = next_state\n",
        "        if done or trunc:\n",
        "          self.trewards=max(self.max_trewards, f)\n",
        "          templ = f'episode={e:4d} |treward={f:4d}'\n",
        "          templ += f'|max={self.max_trewards:4d}'\n",
        "          print(templ, end='\\r')\n",
        "          break\n",
        "      if len(self.memory)> self.batch_size:\n",
        "                self.replay()\n",
        "    print()\n",
        "\n",
        "class DQLAgent(DQLAgent):\n",
        "  def test(self, episodes):\n",
        "    for e in range(1, episodes +1):\n",
        "      state, _ = self.env.reset()\n",
        "\n",
        "      state = np.reshape(state, [1,4])\n",
        "      for f in range(1,5001):\n",
        "        action = np.argmax(self.model.predict(state)[0])\n",
        "        state, reward, done, trunc, _= self.env.step(action)\n",
        "        state = np.reshape(state, [1,4]) # Fixed: state was state[1,4] which is incorrect\n",
        "        if done or trunc:\n",
        "          print(f, end=' ')\n",
        "          break"
      ],
      "metadata": {
        "id": "xeTUqDWzamdJ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQLAgent()\n",
        "# %time agent.learn(1500)\n",
        "# episode=1500 | treward = 224 | max = 500\n",
        "# CPU times: user 1min 52s, sys:21.7 s, total: 2min 14s\n",
        "# Wall time: 1min 46s\n",
        "\n"
      ],
      "metadata": {
        "id": "CUkHzce-aqDq"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.epsilon\n",
        "agent.test(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "bbOhPw0nfqUV",
        "outputId": "1f9d72d2-2b6d-40aa-c5a1-94c0eafb34f0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3447257709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-398886508.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Fixed: state was state[1,4] which is incorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n\u001b[0m\u001b[1;32m    504\u001b[0m                          \"iteration in eager mode or within tf.function.\")\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15r_hzmSmPhS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}